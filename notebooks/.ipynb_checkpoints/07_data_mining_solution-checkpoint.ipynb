{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE: Association analysis from scratch\n",
    "\n",
    "[Adapted from http://aimotion.blogspot.com.au/2013/01/machine-learning-and-data-mining.html.]\n",
    "\n",
    "[For more on efficient approaches, see http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf.]\n",
    "\n",
    "Refer to slides for definitions (itemset, support, frequent itemset, confidence, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate frequent itemsets\n",
    "\n",
    "Let's find all sets of items with a support greater than some threshold.\n",
    "\n",
    "We define 4 functions for generating frequent itemsets:\n",
    "* createC1 - Create first candidate itemsets for k=1\n",
    "* scanD - Identify itemsets that meet the support threshold\n",
    "* aprioriGen - Generate the next list of candidates\n",
    "* apriori - Generate all frequent itemsets\n",
    "\n",
    "See slides for explanation of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createC1(dataset):\n",
    "    \"Create a list of candidate item sets of size one.\"\n",
    "    c1 = []\n",
    "    for transaction in dataset:\n",
    "        for item in transaction:\n",
    "            if not [item] in c1:\n",
    "                c1.append([item])\n",
    "    c1.sort()\n",
    "    #frozenset because it will be a ket of a dictionary.                         \n",
    "    return list(map(frozenset, c1))\n",
    "\n",
    "\n",
    "def scanD(dataset, candidates, min_support):\n",
    "    \"Returns all candidates that meets a minimum support level\"\n",
    "    sscnt = {}\n",
    "    for tid in dataset:\n",
    "        for can in candidates:\n",
    "            if can.issubset(tid):\n",
    "                sscnt.setdefault(can, 0)\n",
    "                sscnt[can] += 1\n",
    "\n",
    "    num_items = float(len(dataset))\n",
    "    retlist = []\n",
    "    support_data = {}\n",
    "    for key in sscnt:\n",
    "        support = sscnt[key] / num_items\n",
    "        if support >= min_support:\n",
    "            retlist.insert(0, key)\n",
    "        support_data[key] = support\n",
    "    return retlist, support_data\n",
    "\n",
    "\n",
    "def aprioriGen(freq_sets, k):\n",
    "    \"Generate the joint transactions from candidate sets\"\n",
    "    retList = []\n",
    "    lenLk = len(freq_sets)\n",
    "    for i in range(lenLk):\n",
    "        for j in range(i + 1, lenLk):\n",
    "            L1 = list(freq_sets[i])[:k - 2]\n",
    "            L2 = list(freq_sets[j])[:k - 2]\n",
    "            L1.sort()\n",
    "            L2.sort()\n",
    "            if L1 == L2:\n",
    "                retList.append(freq_sets[i] | freq_sets[j]) # | is set union\n",
    "    return retList\n",
    "\n",
    "\n",
    "def apriori(dataset, min_support=0.5):\n",
    "    \"Generate a list of candidate item sets\"\n",
    "    C1 = createC1(dataset)\n",
    "    D = list(map(set, dataset))\n",
    "    L1, support_data = scanD(D, C1, min_support)\n",
    "    L = [L1]\n",
    "    k = 2\n",
    "    while (len(L[k - 2]) > 0):\n",
    "        Ck = aprioriGen(L[k - 2], k)\n",
    "        Lk, supK = scanD(D, Ck, min_support)\n",
    "        support_data.update(supK)\n",
    "        L.append(Lk)\n",
    "        k += 1\n",
    "\n",
    "    return L, support_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Itemset generation on sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Dataset in list-of-lists format:\\n', [[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]], '\\n')\n",
      "('Initial 1-itemset candidates:\\n', [frozenset([1]), frozenset([2]), frozenset([3]), frozenset([4]), frozenset([5])], '\\n')\n",
      "('Dataset in list-of-sets format:\\n', [set([1, 3, 4]), set([2, 3, 5]), set([1, 2, 3, 5]), set([2, 5])], '\\n')\n",
      "('1-itemsets that appear in at least 50% of transactions:\\n', [frozenset([1]), frozenset([3]), frozenset([2]), frozenset([5])], '\\n')\n",
      "('Next set of candidates:\\n', [frozenset([1, 3]), frozenset([1, 2]), frozenset([1, 5]), frozenset([2, 3]), frozenset([3, 5]), frozenset([2, 5])], '\\n')\n",
      "('Full list of candidate itemsets:\\n', [[frozenset([1]), frozenset([3]), frozenset([2]), frozenset([5])], [frozenset([1, 3]), frozenset([2, 5]), frozenset([2, 3]), frozenset([3, 5])], [frozenset([2, 3, 5])], []], '\\n')\n",
      "('Support values for candidate itemsets:\\n', {frozenset([5]): 0.75, frozenset([3]): 0.75, frozenset([2, 3, 5]): 0.5, frozenset([1, 2]): 0.25, frozenset([1, 5]): 0.25, frozenset([3, 5]): 0.5, frozenset([4]): 0.25, frozenset([2, 3]): 0.5, frozenset([2, 5]): 0.75, frozenset([1]): 0.5, frozenset([1, 3]): 0.5, frozenset([2]): 0.75}, '\\n')\n"
     ]
    }
   ],
   "source": [
    "MIN_SUPPORT=0.5\n",
    "\n",
    "# Sample data\n",
    "DATASET = [[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]]\n",
    "print('Dataset in list-of-lists format:\\n', DATASET, '\\n')\n",
    "\n",
    "# Generate a first candidate itemsets for k=1\n",
    "C1 = createC1(DATASET)\n",
    "print('Initial 1-itemset candidates:\\n', C1, '\\n')\n",
    "\n",
    "# Convert data to a list of sets\n",
    "D = list(map(set, DATASET))\n",
    "print('Dataset in list-of-sets format:\\n', D, '\\n')\n",
    "\n",
    "# Identify items that meet support threshold (0.5)\n",
    "# Note that {4} isn't here as it only occurs in one transaction.\n",
    "# Remove it so we don't generate any further candidate itemsets containing {4}.\n",
    "L1, support_data = scanD(D, C1, MIN_SUPPORT)\n",
    "print('1-itemsets that appear in at least 50% of transactions:\\n', L1, '\\n')\n",
    "\n",
    "# Generate the next list of candidates\n",
    "print('Next set of candidates:\\n', aprioriGen(L1,2), '\\n')\n",
    "\n",
    "# Generate all candidate itemsets\n",
    "L, support_data = apriori(DATASET, min_support=MIN_SUPPORT)\n",
    "print('Full list of candidate itemsets:\\n', L, '\\n')\n",
    "print('Support values for candidate itemsets:\\n', support_data, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Exploring support thresholds\n",
    "\n",
    "* Generate frequent itemsets with a support threshold of 0.7\n",
    "* How many frequent itemsets do we get at 0.7?\n",
    "* How many do we get at 0.3?\n",
    "* What would be a reasonable value for supermarket transaction data?\n",
    "* Do you have datasets that resemble transactions?\n",
    "* What about the apps/websites you use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 - \n",
    "l0_7, sd0_7 = apriori(DATASET, min_support=0.7)\n",
    "print('Full list of candidate itemsets:\\n', l0_7, '\\n')\n",
    "print('Support values for candidate itemsets:\\n', sd0_7, '\\n')\n",
    "\n",
    "# 2 - \n",
    "print('Number of frequent itemsets at 0.7:', len([i for ksets in l0_7 for i in ksets]))\n",
    "\n",
    "# 3 - \n",
    "l0_3, sd0_3 = apriori(DATASET, min_support=0.3)\n",
    "print('Number of frequent itemsets at 0.3:', len([i for ksets in l0_3 for i in ksets]))\n",
    "\n",
    "# 4 - Much lower (e.g., 5%) to actually generate any frequent itemsets on real data\n",
    "\n",
    "# 5 - Could imagine doing this for files to know what tends to be open at the same time.\n",
    "\n",
    "# 6 - Many, many! E.g., Amazon, Netflix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mine association rules\n",
    "\n",
    "Given frequent itemsets, we can create association rules.\n",
    "\n",
    "We add three more functions:\n",
    "* calc_confidence - Identify rules that meet the confidence threshold\n",
    "* rules_from_conseq - Recursively generate and evaluate candidate rules\n",
    "* generateRules - Mine all confident association rules\n",
    "\n",
    "See slides for explanation of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_confidence(freqSet, H, support_data, rules, min_confidence=0.7):\n",
    "    \"Evaluate the rule generated\"\n",
    "    pruned_H = []\n",
    "    for conseq in H:\n",
    "        conf = support_data[freqSet] / support_data[freqSet - conseq]\n",
    "        if conf >= min_confidence:\n",
    "            #print(freqSet - conseq, '--->', conseq, 'conf:', conf)\n",
    "            rules.append((freqSet - conseq, conseq, conf))\n",
    "            pruned_H.append(conseq)\n",
    "    return pruned_H\n",
    "\n",
    "\n",
    "def rules_from_conseq(freqSet, H, support_data, rules, min_confidence=0.7):\n",
    "    \"Generate a set of candidate rules\"\n",
    "    m = len(H[0])\n",
    "    if (len(freqSet) > (m + 1)):\n",
    "        Hmp1 = aprioriGen(H, m + 1)\n",
    "        Hmp1 = calc_confidence(freqSet, Hmp1,  support_data, rules, min_confidence)\n",
    "        if len(Hmp1) > 1:\n",
    "            rules_from_conseq(freqSet, Hmp1, support_data, rules, min_confidence)\n",
    "\n",
    "def generateRules(L, support_data, min_confidence=0.7):\n",
    "    \"\"\"Create the association rules\n",
    "    L: list of frequent item sets\n",
    "    support_data: support data for those itemsets\n",
    "    min_confidence: minimum confidence threshold\n",
    "    \"\"\"\n",
    "    rules = []\n",
    "    for i in range(1, len(L)):\n",
    "        for freqSet in L[i]:\n",
    "            H1 = [frozenset([item]) for item in freqSet]\n",
    "            print(\"freqSet\", freqSet, 'H1', H1)\n",
    "            if (i > 1):\n",
    "                rules_from_conseq(freqSet, H1, support_data, rules, min_confidence)\n",
    "            else:\n",
    "                calc_confidence(freqSet, H1, support_data, rules, min_confidence)\n",
    "    return rules\n",
    "\n",
    "def print_rules(rules):\n",
    "    for r in rules:\n",
    "        print('{} ==> {} (c={})'.format(*r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule mining on sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('freqSet', frozenset([1, 3]), 'H1', [frozenset([1]), frozenset([3])])\n",
      "('freqSet', frozenset([2, 5]), 'H1', [frozenset([2]), frozenset([5])])\n",
      "('freqSet', frozenset([2, 3]), 'H1', [frozenset([2]), frozenset([3])])\n",
      "('freqSet', frozenset([3, 5]), 'H1', [frozenset([3]), frozenset([5])])\n",
      "('freqSet', frozenset([2, 3, 5]), 'H1', [frozenset([2]), frozenset([3]), frozenset([5])])\n",
      "frozenset([1]) ==> frozenset([3]) (c=1.0)\n",
      "frozenset([5]) ==> frozenset([2]) (c=1.0)\n",
      "frozenset([2]) ==> frozenset([5]) (c=1.0)\n"
     ]
    }
   ],
   "source": [
    "MIN_CONFIDENCE=0.7\n",
    "\n",
    "# Mine association rules\n",
    "association_rules = generateRules(L, support_data, min_confidence=MIN_CONFIDENCE)\n",
    "print_rules(association_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Exploring confidence thresholds\n",
    "\n",
    "* Mine rules with a confidence threshold of 0.9\n",
    "* How many rules do we get at 0.9?\n",
    "* How many do we get at 0.5?\n",
    "* What would be a reasonable value for supermarket transaction data?\n",
    "* Can we use this for recommendation (e.g., Amazon, Netflix)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 - \n",
    "r0_9 =  generateRules(L, support_data, min_confidence=0.9)\n",
    "print('Rules for confidence threshold of 0.9:')\n",
    "print_rules(r0_9)\n",
    "\n",
    "# 2 - \n",
    "print('Number of rules at 0.9:', len(r0_9))\n",
    "\n",
    "# 3 - \n",
    "r0_5 =  generateRules(L, support_data, min_confidence=0.5)\n",
    "print('Rules for confidence threshold of 0.5:')\n",
    "print_rules(r0_5)\n",
    "print('Number of rules at 0.5:', len(r0_5))\n",
    "\n",
    "# 4 - 70% might be reasonable; it will depend on the data and how many rules the business can use\n",
    "\n",
    "# 5 - Absolutely, especially in session-focused recommendation ignoring user profile and history.\n",
    "#     [https://en.wikipedia.org/wiki/Recommender_system]\n",
    "#     [https://www.quora.com/How-does-Amazons-collaborative-filtering-recommendation-engine-work]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *STOP PLEASE. THE FOLLOWING IS FOR THE NEXT EXERCISE. THANKS.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE: Clustering with k-means\n",
    "\n",
    "[Adapted from http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html#example-cluster-plot-kmeans-digits-py]\n",
    "\n",
    "### Loading handwritten digits data\n",
    "\n",
    "We'll work with the handwritten digits dataset, a classic machine-learning dataset used to explore automatic recognition of handwritten digits (i.e., 0, 1, 2, ..., 9).\n",
    "\n",
    "For more information:\n",
    "* http://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits\n",
    "* http://scikit-learn.org/stable/tutorial/basic/tutorial.html#loading-an-example-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The digits data comprises 1797 64-dimensional representations of handwritten digits:\n",
      "[[  0.   0.   5. ...,   0.   0.   0.]\n",
      " [  0.   0.   0. ...,  10.   0.   0.]\n",
      " [  0.   0.   0. ...,  16.   9.   0.]\n",
      " ..., \n",
      " [  0.   0.   1. ...,   6.   0.   0.]\n",
      " [  0.   0.   2. ...,  12.   0.   0.]\n",
      " [  0.   0.  10. ...,  12.   1.   0.]]\n",
      "\n",
      "It also includes labels:\n",
      "[0 1 2 ..., 8 9 8]\n",
      "\n",
      "And it includes the original 8x8 image representation:\n",
      "[[  0.   0.   5.  13.   9.   1.   0.   0.]\n",
      " [  0.   0.  13.  15.  10.  15.   5.   0.]\n",
      " [  0.   3.  15.   2.   0.  11.   8.   0.]\n",
      " [  0.   4.  12.   0.   0.   8.   8.   0.]\n",
      " [  0.   5.   8.   0.   0.   9.   8.   0.]\n",
      " [  0.   4.  11.   0.   1.  12.   7.   0.]\n",
      " [  0.   2.  14.   5.  10.  12.   0.   0.]\n",
      " [  0.   0.   6.  13.  10.   0.   0.   0.]]\n",
      "\n",
      "Let's look at a few images:\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAABCCAYAAAB6pUwxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106ba34d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "print('The digits data comprises {} {}-dimensional representations of handwritten digits:\\n{}\\n'.format(\n",
    "        digits.data.shape[0],\n",
    "        digits.data.shape[1],\n",
    "        digits.data\n",
    "    ))\n",
    "\n",
    "print('It also includes labels:\\n{}\\n'.format(digits.target))\n",
    "\n",
    "print('And it includes the original 8x8 image representation:\\n{}\\n'.format(digits.images[0]))\n",
    "\n",
    "print('Let\\'s look at a few images:\\n')\n",
    "NUM_SUBPLOT_ROWS = 1\n",
    "NUM_SUBPLOT_COLS = 8\n",
    "for i in range(NUM_SUBPLOT_ROWS*NUM_SUBPLOT_COLS):\n",
    "    _ = plt.subplot(NUM_SUBPLOT_ROWS,NUM_SUBPLOT_COLS,i+1)\n",
    "    _ = plt.imshow(digits.images[i], cmap=plt.cm.gray_r, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering handwritten digits\n",
    "\n",
    "That's the data. Now let's try clustering these 64d vectors.\n",
    "\n",
    "`scikit-learn` implements many differnt machine learning algorithms.\n",
    "\n",
    "The normal pattern is to:\n",
    "1. intialise an estimator (e.g., `estimator = KMeans()`)\n",
    "1. fit to the training data (e.g., `estimator.fit(training_data)`)\n",
    "1. label the test data (e.g., `estimator.predict(test_data)`)\n",
    "\n",
    "For clustering, we don't have separate training and test data.\n",
    "\n",
    "So the labelling is created when we fit and accessed by `estimator.labels_`. Note that, for clustering, these are cluster IDs. They are NOT labels.\n",
    "\n",
    "`estimator.inertia_` gives the sum of squared errors (SSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled digits data:\n",
      "[[ 0.         -0.33501649 -0.04308102 ..., -1.14664746 -0.5056698\n",
      "  -0.19600752]\n",
      " [ 0.         -0.33501649 -1.09493684 ...,  0.54856067 -0.5056698\n",
      "  -0.19600752]\n",
      " [ 0.         -0.33501649 -1.09493684 ...,  1.56568555  1.6951369\n",
      "  -0.19600752]\n",
      " ..., \n",
      " [ 0.         -0.33501649 -0.88456568 ..., -0.12952258 -0.5056698\n",
      "  -0.19600752]\n",
      " [ 0.         -0.33501649 -0.67419451 ...,  0.8876023  -0.5056698\n",
      "  -0.19600752]\n",
      " [ 0.         -0.33501649  1.00877481 ...,  0.8876023  -0.26113572\n",
      "  -0.19600752]]\n",
      "\n",
      "('Sum of squared errors:', 69417.009107181017)\n",
      "('Clusters from k-means:', array([7, 5, 5, 3, 6, 3, 1, 9, 3, 3], dtype=int32))\n",
      "('Gold standard classes:', array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import scale\n",
    "import numpy as np\n",
    "\n",
    "# First let's scale the digits data (center to mean and scale to unit variance)\n",
    "data = scale(digits.data)\n",
    "print('Scaled digits data:\\n{}\\n'.format(data))\n",
    "\n",
    "# Let's grab the data we'll need\n",
    "n_samples, n_features = data.shape\n",
    "n_digits = len(np.unique(digits.target)) # classes\n",
    "labels = digits.target\n",
    "\n",
    "# And let's run k-means, specifying initialisation (k-means++), k (n_digits),\n",
    "# and the number of runs (10)\n",
    "estimator = KMeans(init='k-means++', n_clusters=n_digits, n_init=10)\n",
    "estimator.fit(data)\n",
    "print('Sum of squared errors:', estimator.inertia_)\n",
    "print('Clusters from k-means:', estimator.labels_[:10])\n",
    "print('Gold standard classes:', labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Try different initialisations\n",
    "\n",
    "Initialisation has a large effect on cluster output. Let's try a few options.\n",
    "\n",
    "* Initialise with random (`init='random'`)\n",
    "* Run PCA with k components (`pca = PCA(n_components=n_digits).fit(data)`)\n",
    "* Use PCA components to initialise `KMeans` (`init=pca.components_`)\n",
    "* Can we determine which approach is best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM INITIALISATION\n",
      "('Num of squared errors:', 69865.653191838268)\n",
      "('Clusters from k-means:', array([5, 7, 7, 9, 2, 9, 1, 4, 9, 9], dtype=int32))\n",
      "('Gold standard classes:', array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))\n",
      "\n",
      "INITIALISATION WITH PCA COMPONENTS\n",
      "('Num of squared errors:', 71820.930407889362)\n",
      "('Clusters from k-means:', array([2, 9, 9, 3, 0, 3, 4, 1, 3, 3], dtype=int32))\n",
      "('Gold standard classes:', array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 - \n",
    "est_random = KMeans(init='random', n_clusters=n_digits, n_init=10)\n",
    "est_random.fit(data)\n",
    "print('RANDOM INITIALISATION')\n",
    "print('Num of squared errors:', est_random.inertia_)\n",
    "print('Clusters from k-means:', est_random.labels_[:10])\n",
    "print('Gold standard classes:', labels[:10])\n",
    "print('')\n",
    "\n",
    "# 2 - \n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=n_digits).fit(data)\n",
    "\n",
    "# 3 - \n",
    "est_pca = KMeans(init=pca.components_, n_clusters=n_digits, n_init=1)\n",
    "est_pca.fit(data)\n",
    "print('INITIALISATION WITH PCA COMPONENTS')\n",
    "print('Num of squared errors:', est_pca.inertia_)\n",
    "print('Clusters from k-means:', est_pca.labels_[:10])\n",
    "print('Gold standard classes:', labels[:10])\n",
    "print('')\n",
    "\n",
    "# 4 - It looks like k-means++ >> random >> PCA from SSE/inertia.\n",
    "#     But SSE is an internal validation measure.\n",
    "#     Since we're trying to cluster by digit, we can't really say\n",
    "#     which is best without comparing to the gold partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *STOP PLEASE. THE FOLLOWING IS FOR THE NEXT EXERCISE. THANKS.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE: Evaluating clustering\n",
    "\n",
    "Since we have a gold-standard labels, we can compare our system clustering to the true partition.\n",
    "\n",
    "`scikit-learn` includes various metrics for this:\n",
    "* Homogeneity\n",
    "* Completeness\n",
    "* V-measure\n",
    "* Adjusted Rand index (ARI)\n",
    "* Adjusted mutual information (AMI)\n",
    "* Silhouette coefficient\n",
    "\n",
    "For more information:\n",
    "* http://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation\n",
    "\n",
    "Let's compare the above clusterings using V-measure.\n",
    "\n",
    "Note that you may need to re-estimate or rename `est_random` and `est_pca` from the last exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('k-means++ initialisation:', 0.62759639152686164)\n",
      "('random initialisation:   ', 0.6777181043537035)\n",
      "('pca initialisation:      ', 0.69322745961589083)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print('k-means++ initialisation:', metrics.v_measure_score(labels, estimator.labels_))\n",
    "print('random initialisation:   ', metrics.v_measure_score(labels, est_random.labels_))\n",
    "print('pca initialisation:      ', metrics.v_measure_score(labels, est_pca.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing initialisations\n",
    "\n",
    "Let's be a bit more exhastive, comparing initialisations using variuos evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "sample_size = 300\n",
    "\n",
    "def bench_k_means(estimator, name, data):\n",
    "    \"Calculate various metrics for comparing system clustering to a gold partition\"\n",
    "    t0 = time()\n",
    "    estimator.fit(data)\n",
    "    print('% 9s   %.2fs    %i   %.3f   %.3f   %.3f   %.3f   %.3f    %.3f'\n",
    "          % (name, (time() - t0), estimator.inertia_,\n",
    "             metrics.homogeneity_score(labels, estimator.labels_),\n",
    "             metrics.completeness_score(labels, estimator.labels_),\n",
    "             metrics.v_measure_score(labels, estimator.labels_),\n",
    "             metrics.adjusted_rand_score(labels, estimator.labels_),\n",
    "             metrics.adjusted_mutual_info_score(labels,  estimator.labels_),\n",
    "             metrics.silhouette_score(data, estimator.labels_,\n",
    "                                      metric='euclidean',\n",
    "                                      sample_size=sample_size)))\n",
    "\n",
    "# print table header\n",
    "print(75 * '_')\n",
    "print('init         time  inertia    homo   compl  v-meas     ARI     AMI silhouet')\n",
    "print(75 * '_')\n",
    "\n",
    "# benchmark k-means++ initialisation\n",
    "bench_k_means(KMeans(init='k-means++', n_clusters=n_digits, n_init=10),\n",
    "              name=\"k-means++\", data=data)\n",
    "\n",
    "# benchmark random initialisation\n",
    "bench_k_means(KMeans(init='random', n_clusters=n_digits, n_init=10),\n",
    "              name=\"random\", data=data)\n",
    "\n",
    "# benchmark PCA initalisation\n",
    "pca = PCA(n_components=n_digits).fit(data)\n",
    "bench_k_means(KMeans(init=pca.components_, n_clusters=n_digits, n_init=1),\n",
    "              name=\"PCA-based\",\n",
    "              data=data)\n",
    "\n",
    "print(75 * '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Reading evaluation output\n",
    "\n",
    "- Which approach performs best? How would you order the other two?\n",
    "- Do you neighbours get the same result?\n",
    "- Can we apply statistical significance testing?\n",
    "- How else can we test reliability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 - PCA > random, PCA > k-means++, hard to say for random and k-means++\n",
    "\n",
    "# 2 - Run multiple times, k-means clustering depends on initialisation and changes\n",
    "\n",
    "# 3 - Not directly to the clustering output. Clustering is often more of an exploratory tool.\n",
    "\n",
    "# 4 - A few possibilities..\n",
    "#     We could evaluate the impact of different clusterings on another task.\n",
    "#     We could do a bootstrap stability analysis\n",
    "#     (http://www.r-bloggers.com/bootstrap-evaluation-of-clusters/).\n",
    "#     We could use cophenitic correlation for hierarchical clustering\n",
    "#     (https://en.wikipedia.org/wiki/Cophenetic_correlation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *STOP PLEASE. THE FOLLOWING IS FOR THE NEXT EXERCISE. THANKS.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE: Choosing k\n",
    "\n",
    "### Create example data for choosing k\n",
    "\n",
    "First, let's create some example data with 4 clusters using make_blobs.\n",
    "\n",
    "We set `random_state=1` so we all get the same clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generating the sample data from make_blobs\n",
    "# This particular setting has one distict cluster and 3 clusters placed close\n",
    "# together.\n",
    "X, y = make_blobs(n_samples=500,\n",
    "                  n_features=2,\n",
    "                  centers=4,\n",
    "                  cluster_std=1,\n",
    "                  center_box=(-10.0, 10.0),\n",
    "                  shuffle=True,\n",
    "                  random_state=1)  # For reproducibility\n",
    "d1 = X[:,0] # first dimension\n",
    "d2 = X[:,1] # second dimension\n",
    "_ = plt.scatter(d1,d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing k using silhouette analysis\n",
    "\n",
    "[Adapted from http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html]\n",
    "\n",
    "For good clusterings:\n",
    "* the average silhouette should be close to 1 indicating that points are far away from neighbouring clusters \n",
    "* all cluster silhouettes should be close to the average silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "range_n_clusters = range(2,6)\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhoutte score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors)\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1],\n",
    "                marker='o', c=\"white\", alpha=1, s=200)\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Choosing k\n",
    "\n",
    "[Derived from Data Science from Scratch, Chapter 19]\n",
    "\n",
    "The textbook suggests another interactive approach for choosing the number of clusters: plot SSE versus k and looking for the knee (the point where the graph bends).\n",
    "\n",
    "- Plot inertia against k for k from 2 to 6\n",
    "- What k would you choose for each? Is it the same?\n",
    "- Does this work on the handwritten digits code? Why / why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 -\n",
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "inertia_values = [KMeans(n_clusters=k).fit(X).inertia_ for k in range_n_clusters]\n",
    "_ = plt.plot(range_n_clusters, inertia_values)\n",
    "\n",
    "# 2 - From this plot, it looks like the knee is at 4 or maybe 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3 - Nope. The handwritten digits data is difficult to cluster.\n",
    "#     However, we haven't done anything clever with our feature representation.\n",
    "#     We might do better, e.g., with spectral clustering\n",
    "#     (http://scikit-learn.org/stable/modules/clustering.html#spectral-clustering).\n",
    "#     Q: Does spectral clustering outperform \n",
    "#     We leave this as an extra exercise for the keen reader.\n",
    "range_n_clusters = range(5,15)\n",
    "inertia_values = [KMeans(n_clusters=k).fit(data).inertia_ for k in range_n_clusters]\n",
    "_ = plt.plot(range_n_clusters, inertia_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
